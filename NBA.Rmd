---
title: "NBA Career Longevity"
author: "Sahba Salarian"
date: "Feb. 2019"
output: pdf_document
  
---

```{r setup, include=FALSE, echo=FALSE}
library(float)
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_chunk$set(fig.pos='H')
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(message=FALSE)
library(knitr)
library(xtable)
options(xtable.comment = FALSE)
library(tidyverse)
library(plyr)
library(ggplot2)
library(GGally)
library(stargazer)
library(corrplot)
library(car)
library(repr)
library(MASS)
library(leaps)
library(caret)
library(kernlab)
library(e1071)
library(pROC)
library(ROCR)
```

##Introduction

The National Basketball Association (NBA) is a men's professional basketball league in North America. It is composed of 30 teams among which 29 teams are in the United States and 1 team in Canada. It is widely considered to be the premier men's professional basketball league in the world. 

The NBA is considered one of the four major professional sports leagues in the United States and Canada. NBA players are the world's best paid athletes by average annual salary per player, among which have been Michael Jordan, Kobe Bryant, LeBron James, Kareem Abdul-Jabbar, etc. Considering the huge prevelage gained by these athletes, it is an interesting analysis to invetigate the career longevity of NBA players in the league based on their athlectic performance in the field. 

In this project, the player's career length is devided into two categories of more or less than 5 years, associated with output values of 1 or 0, respectively. 

##Data Explanation

The data set provides information about field performance of each player, from 1980 to 2016. It consists of 1340 observations with 21 variables. The values for each variable except for the career longevity is calculated as mean per game during the associated rookie year. 

The original source of the dataset is the official website for the National Bascketbal Association (www.NBA.com) and the current dataset was retrieved from the data.world repository for data analysis and data competition.

```{r GETDATA, echo=FALSE}
df <-read.csv("/Users/sahba/Dropbox/Data Science/NBA longevity/nba_logreg.csv", header=T, stringsAsFactors=F)
```

The list of all the variables are shown in Table 1.

##Dataset Information

```{r data structure, echo = TRUE}
str(df) 
```

The TARGET_5Yrs should be analyzed as a binary class, versus other variables for each athlete. The variables are demonstrated in Table 1.

\begin{table}[h]
\centering
	\begin{tabular}{ c c }
		\hline\hline
			
		 Column Names& Explanation   \\
		\hline
		name & ASCII subject name and recording number\\ 
Name  & Name \\
GP    & Games Played \\
MIN   & Minutes Played \\
PTS   & Points Per Game \\
FGM   & Field Goals Made \\
FGA   & Field Goals Attempts \\
FG.   & Field Goals Percent \\
X3P.Made & 3Points Made \\
X3PA  & 3Points Attempts\\ 
X3P.  & 3Points Attempts Percentage\\ 
FTM   & Free Throw Made \\
FTA   & Free throw Attempts \\
FT.   & Free throw Percenrage \\
OREB  & Offensive Rebounds\\
DREB  & Defenive Rebounds\\
REB   & Rebounds\\
AST   & Assists\\
STL   & Steals\\
BLK   & Blocks\\
TOV   & Turnovers\\
TARGET-5Yrs & Outcome=1(career length>=5 yrs),Outcome=0(career length<5)\\ 
		\hline
	\end{tabular}
	\caption{Attribute Information}
	\label{tab1}
\end{table}

\newpage
## Data Engineering 

In this stage the NA values of the data set are detedcted and the rows with such unknown or missing values are omitted from the data set. Moreover, the Name column is also removed from the dataset. 1329 observations and 20 variables are remaining as follows:

```{r Omit NA , echo = FALSE}
df <- na.omit(df)
#creating a data set without names:
df <- df[,2:21]
```


```{r trimmed data , echo = FALSE}
#str(df)
```
##Train \& Test Split:

Just because a learning algorithm fits a data set well, does not guarantee that it is a good hypothesis. It could overfit and as a result the predictions on the other data set would be poor. The error of hypothesis as measured on the data set with which we trained the parameters will usually be lower than the errors on any other data set. So for the sake of better evaluation of the model and better prediction analysis the data set is randomly splitted into two separate datasets of "train" and "test", with 70\% and 30\% of the whole data, respectively. The train and test datasets respectively have 921 and 408 observatinos with 20 variables.

```{r initialize, echo=FALSE}
ETA <- numeric(4)
CI <- matrix(numeric(4*2), nrow=4, ncol=2)
```

```{r test&train, echo = FALSE}
set.seed(798102) 
split <- sample (2, nrow (df), replace= TRUE, prob = c (0.7, 0.3))
train <- df [split==1,]
test <- df [split==2,]
```

##Correlation Matrix

At the very first step, because of the importance and to get a better understanding of the dataset, correlation matrix for the predictors is plotted. 
The dark blue color between not-identical variables shown in Figure 1, demonstrates some non-negligible input feature correlations. 

```{r Correlation Matrix, echo=FALSE,fig.align='center', fig.cap="Correlation Matrix"}
correlationMatrix<- cor(train[,])
#Figure 1
corrplot(correlationMatrix, type="lower")
```
The distribution and scatterplot matrix has been added to have a better understanding of the correlation between the features.

```{r Scatter-CORRELATION-PLOT2, echo=FALSE, fig.width=22, fig.height=22, fig.cap = "Scatterplot matrix for predictors"}
#Figure 2
ggpairs(df, aes(color=factor(TARGET_5Yrs), alpha=0.75), lower=list(continuous="smooth"))+ 
  theme_bw()+
 labs(title="feature scatterplot")+
  theme(plot.title=element_text(face='bold',color='black',hjust=0.5,size=12))
```

## Regarding the correlated features

As illustrated in correlation and scatterplot matrices, some features are very correlated with each other. Different methods exist for omitting the collinearity, e.g. removing the set of correlated features and just keeping those more correlated with outcome or removing the features with lower variances. In this project the **caret** package has been used to find the best trained model based on automated training package in R.


##Fitting simple logistics regression model:

A simple logistic regression model with all inputs is fitted to the dataset, the summary of the fit, deviance, residual deviance and Chi-squared test analysis are shown in Table 2. As illustrated in Table 2, many features with high Pr(>Chi) are insignificant predictors.


```{r Logistic model, echo=FALSE}
LogFit <- glm(TARGET_5Yrs ~  GP+ MIN+ PTS+ FGM+ FGA+ FG.+X3P.Made+ X3PA+ X3P.+FTM+ FTA+ FT.+OREB +DREB +REB+ AST+ STL+ BLK+ TOV , data=train, family=binomial(link='logit'))

#compute pseudo-R squared
DMLE <- LogFit$deviance
DNULL <- LogFit$null.deviance
n <- nrow(train)
Rsq <- (1-exp((DMLE-DNULL)/n)) / (1-exp(-DNULL/n))
yH <- ifelse(predict(LogFit, type="response") < 0.5, "0", "1") 
yTe <- factor(test$TARGET_5Yrs)
eta <- mean(yTe!=yH) 
#comparisons
MOE <- 1.96*sqrt(eta*(1-eta)/length(yH))
CI[1,] <- round(100*(eta+c(-1,1)*MOE), 2)
ETA[1] <- eta

```


```{r LogFit summary-Chisq, echo=FALSE, results='asis' , header = FALSE, fig.align='center'}
#Table 2
print(xtable(anova(LogFit, test="Chisq"), 
      caption="summary of logistic regression fit with all inputs"), 
      type="latex", caption.placement="top")
```

Pseudo-R squared and mis-classification rate are also computed for the logistic fit, demonstrated in Table 3.

```{r LogFit accuracy, echo=FALSE, echo=FALSE, results='asis' , header = FALSE}
rsqr <-round(100*Rsq,1)
eta  <-eta
accur <- round(100*(1-eta),1)
#Table3
co <- data.frame( Parametrs=c("R-squared", " eta", "accuracy" ) , Values=c( rsqr, eta, accur))
stargazer(t(co), type="latex" , header= FALSE, title= "simple logistic model accuracy for train dataset")
```
\newpage
## Using  **caret** for Training Machine Learning Models

The **caret** package in R, is a complete package used to train different machine learning algorithms. For applying the training procedure a trainControl(), cross validation method, shall be defined. The train cotrol for this project has been set on "bootstrap"  with 25 as the number, the default setting in R, with TRUE probability class. Although there is a chance that bootstrap method gives underfit results in some cases, this becomes negligible when the observations are large enough. Since there are 1329 observations with 19 predictors in this project, the chance of underfit results can be neglected.

##Use GLM to fit the model
The first model trained in this project is GLM method, applied on the train test by using the **caret::train()** function.

```{r caret initialization, echo=FALSE}
#caret default
caretControl <- trainControl(method = "boot",number = 25, classProbs =  TRUE)
```

```{r level adjustment, echo=FALSE}

train$TARGET_5Yrs <- revalue(as.factor(train$TARGET_5Yrs), c("0"="zero", "1"="one"))
test$TARGET_5Yrs <- revalue(as.factor(test$TARGET_5Yrs), c("0"="zero", "1"="one"))
#mapvalues(train$TARGET_5Yrs, from = c("0", "1"), to = c("zero", "one"))

```

```{r trian GLM, echo=FALSE, results='asis' , header = FALSE}
set.seed(3233)
modelGLM <- train(as.factor(TARGET_5Yrs)~., data = train, method = "glm",
                 trControl=caretControl)
modelGLM

```
over the train dataset. The fit coefficients are showin in Table 4.
```{r modelGlM summary, echo = FALSE, results='asis' , header = FALSE}
#table4
print(xtable(summary(modelGLM), 
      caption="Trained GLM with all inputs-bootstrap"), 
      type="latex", caption.placement="top")
```

Based on the summary of the GLM model over all inputs, the features of PTS, FGM, FGA, X3P.,FTM, FTA,FT.,OREB, DRE, REB, STL and TOV, are shown insignificant.  
Considering the consideration of insignificant features in this model the next investigated model is the AIC model to check the chance of feature removal.

Over the train dataset, based on the confusion matrix, Table 5, the Accuracy for Caret/GLM model is 0.7372 with Kappa 0.4129.

```{r GLM Confusion Matrix-Train, echo=FALSE, results='asis' , header = FALSE }
#turn probabilities into classess and look at their frequencies:
#Table 5- confusion, train
p_modelGLM_train <- predict(modelGLM, train, type="prob")
p_ClassGLM_train <- predict(modelGLM, train)
out0<- table(p_ClassGLM_train, factor(train$TARGET_5Yrs))
outx0 <- xtable(out0, caption="GLM Confusion Matrix-train")
print(outx0, caption.placement="top")
#confusionMatrix(p_ClassGLM_train, factor(train$TARGET_5Yrs))
```

The confusion matrix for GLM model for the test datset is also presented in following table, Table 6. For the test dataset the accuracy is 65.93\% and kappa value is 27.43\%

```{r GLM Confusion Matrix-test, echo=FALSE, results='asis' , header = FALSE }
#turn probabilities into classess and look at their frequencies:
#Table 6
p_modelGLM_test <- predict(modelGLM, test, type="prob")
p_ClassGLM_test <- predict(modelGLM, test)
out1<- table(p_ClassGLM_test, factor(test$TARGET_5Yrs))
outx1 <- xtable(out1, caption="GLM Confusion Matrix-test")
print(outx1, caption.placement="top")
#confusionMatrix(p_ClassGLM_test, factor(test$TARGET_5Yrs))
```


##Using glmStepAIC to fit the model.


```{r AIC, echo=FALSE, warning=FALSE,include=FALSE}
set.seed(3233)
modelAIC <- train(factor(TARGET_5Yrs)~., data = train, method = "glmStepAIC",
                 trControl=caretControl)
#summary(modelAIC)
modelAIC
```

As it is illustrated in the trained AIC model summary table, Table 7. All the final features, the intercept, GP, MIN, FGA, X3P.Made, X3PA, FT., DREB, REB,AST and BLK are in the 95\% significance interval.  

```{r modelAIC summary, echo = FALSE, results='asis' , header = FALSE}
#table7
print(xtable(summary(modelAIC), 
      caption="Fit coefficients for trained AIC-bootstrap"), 
      type="latex", caption.placement="top")
```
Based on the confusion matrix for train dataset, Table 8, the Caret/AIC model has accuracy of 0.7372 and  Kappa value of 0.4122. 


```{r, echo=FALSE}
#str(modelAIC)
```


```{r AIC Confusion Matrix-Train, echo=FALSE, results='asis' , header = FALSE }
#turn probabilities into classess and look at their frequencies:
#Table 8- confusion, train
p_modelAIC_train <- predict(modelAIC, train, type="prob")
p_ClassAIC_train <- predict(modelAIC, train)
out2<- table(p_ClassAIC_train, factor(train$TARGET_5Yrs))
outx2 <- xtable(out2, caption="AIC Confusion Matrix-train")
print(outx2, caption.placement="top")
#confusionMatrix(p_ClassAIC_train, factor(train$TARGET_5Yrs))
```


Confusion Matrix for AIC model, Table 9, shows 65.93\% accuracy for the test dataset with 27.43\% kappa. 


```{r AIC confusion-test, echo=FALSE, warning=FALSE, results='asis' , header = FALSE}
#turn probabilities into classess and look at their frequencies:
#Table 9
p_modelAIC_test <- predict(modelAIC, test, type = "prob")
p_ClassAIC_test<- predict(modelAIC, test)
out3<- table(p_ClassAIC_test, factor(test$TARGET_5Yrs))
outx3 <- xtable(out3, caption="AIC Confusion Matrix-test")
print(outx3, caption.placement="top")
#confusionMatrix(p_ClassAIC_test, factor(test$TARGET_5Yrs))
```



\newpage

##Support Vector Machine method

In this section svm model is used combined with **caret::train** function. since there exist a chnce of linear decision boundary for our dataset we consider SVM fit with both linear and radial kernels and investigate the results. 

##SVM with Linear kernel
Linear kernel is considered for svm fit with R's default train control method, boostrap and TRUE probability and scaling. Scaling is an important factor in SVM since normalization of variables leads to much uniform and stable computations.
```{r modelSVM Linear, warning=FALSE,echo=FALSE}
set.seed(3233)
modelSVMLinear <- train(factor(TARGET_5Yrs)~., data = train, method = "svmLinear",
                 trControl=caretControl, Scale =  TRUE)
modelSVMLinear
```

As mentioned the accuracy with linear kernel in SVM fit, over the trained set is 71.25\% with kappa 35.36\%. Confusion matrix for the model over the train dataset, Table 10, shows accuracy 0.7351 and kappa Kappa of 0.3989. 

```{r confusion Matrix SVMLinear-train, echo=FALSE, results='asis' , header = FALSE}
#Table 10
p_modelSVMLinear_train <- predict(modelSVMLinear, train, "prob")
p_ClassSVMLinear_train <- predict(modelSVMLinear, train)
out4<- table(p_ClassSVMLinear_train, factor(train$TARGET_5Yrs))
outx4 <- xtable(out4, caption="SVM with linear kernel Confusion Matrix-train")
print(outx4, caption.placement="top")
#confusionMatrix(p_ClassSVMLinear_train, factor(train$TARGET_5Yrs))
```

Also, the condusion matrix in Table 11, shows accuracy of 66.91\% for test data set with kappa 28.31\%.
```{r confusion Matrix SVMLinear-test, echo=FALSE, results='asis' , header = FALSE}
#Table 11
p_modelSVMLinear_test <- predict(modelSVMLinear, test, "prob")
p_ClassSVMLinear_test <- predict(modelSVMLinear, test)
out5<- table(p_ClassSVMLinear_test, factor(test$TARGET_5Yrs))
outx5 <- xtable(out5, caption="SVM with linear kernel Confusion Matrix-test")
print(outx5, caption.placement="top")
#confusionMatrix(p_ClassSVMLinear_test, factor(test$TARGET_5Yrs))
```
\newpage

##SVM with radial kernel:

The default and usual kernel for SVM fit is the gaussian or radial kernel. SVM fit with radial kernel is also investigate din this project. 
```{r trian SVM Radial, echo=FALSE}
set.seed(3233)
modelSVMRadial <- train(factor(TARGET_5Yrs)~., data = train, method = "svmRadial",
                 trControl=caretControl, Scale = TRUE)
summary(modelSVMRadial)
modelSVMRadial
```

The model gives accuracy of 71.39\% with kappa 35.79\% for train test. The tunning features, sigma and cost based on the mentioned settings were set on 0.065 and 0.25, respectively. Confusion matrix of the train set illustrated in Table 12, gives accuracy of 0.7362 and Kappa of 0.3932.   

```{r confusion Matrix SVRadial-train, echo=FALSE, results='asis' , header = FALSE}
#Table 12
p_modelSVMRadial_train <- predict(modelSVMRadial, train, "prob")
p_ClassSVMRadial_train <- predict(modelSVMRadial, train)
out6 <- table(p_ClassSVMRadial_train, factor(train$TARGET_5Yrs))
outx6 <- xtable(out6, caption="SVM with radial kernel Confusion Matrix-train")
print(outx6, caption.placement="top")
#confusionMatrix(p_ClassSVMRadial_train, factor(train$TARGET_5Yrs))
```

Confusion matrix for the test set, Table 13, gives 66.67\% accuracy with kappa 26.29\% for SVM with radial kernel.

```{r confusion Matrix SVRadial-test, echo=FALSE, results='asis' , header = FALSE}
#Table 13
p_modelSVMRadial_test <- predict(modelSVMRadial, test, "prob")
p_ClassSVMRadial_test <- predict(modelSVMRadial, test)
out7 <- table(p_ClassSVMRadial_test, factor(test$TARGET_5Yrs))
outx7 <- xtable(out7, caption="SVM with radial kernel Confusion Matrix-test")
print(outx7, caption.placement="top")
#confusionMatrix(p_ClassSVMRadial_test, factor(test$TARGET_5Yrs))
```

\newpage

## Comparison between the different trained models:

The comparison is made between the trained GLM, AIC, SVM with linear kernel and SVM with radial kernel fits, by using **resample()** function in R. The summary of the comparison is illustrated as following.

```{r COMPARISON GLM, AIC, SVMlinear and SVMradial, echo=FALSE}
library(resample)
comparison<-resamples(list(GLM=modelGLM, AIC=modelAIC, SVMLinear=modelSVMLinear, SVMRadial=modelSVMRadial))
summary(comparison)
```


## ROC for the different fitted models
As another evaluation method for finding the best fit, the ROC curve has been drawn for all the fitted models in this project and the AUC values are calculated for each fit. AUC value which is the area under the ROC curve, can be interpreted as the probability that a random chosen instance from Y=1 population will have a higher score than a randomly chosen instance from Y=0 population. This parameter provides a measure for investigating the goodness of prediction.

First we have the simple logistic model with full features, it has AUC value of 0.732. The ROC for simple logistic curve is shown in Figure 3.

```{r ROC for Logistic fit, echo=FALSE,  results='asis' , header = FALSE, fig.align='center', fig.cap="ROC for simple Logistic Fit"}
#Figure 3
p_LogFit<- predict(LogFit,test,type="response")
test_roc = roc(test$TARGET_5Yrs~p_LogFit, plot = TRUE, print.auc = TRUE)
```

ROC for the GLM model trained by caret package, illustrated in Figure 4, presents similar AUC value to the simple logistic regression fit, meaning that the two models can be considered equal.
```{r ROC for GLM, echo=FALSE, results='asis' , header = FALSE, fig.align='center', fig.cap="ROC for GLM"}
#Figure 4
ROC_GLM <- roc(factor(test$TARGET_5Yrs)~p_modelGLM_test[,2], plot = TRUE, print.auc = TRUE)
```

ROC for the fit with AIC model, differs in AUC value, Figure 5 shows AUC=0.738 for this model. A slightly better prediction for the test detaset.
```{r ROC for AIC, echo=FALSE, results='asis' , header = FALSE, fig.align='center', fig.cap="ROC for AIC"}
#Figure 5
ROC_AIC <- roc(factor(test$TARGET_5Yrs)~p_modelAIC_test[,2], plot = TRUE, print.auc = TRUE)
```


By applying the SVM model with linear kernel the area under the curve has improved to AUC=0.742, illustrated in Figure 6.

```{r ROC for SVMlinear, echo=FALSE, results='asis' , header = FALSE, fig.align='center', fig.cap="ROC for SVM with linear kernel"}
#Figure 6
ROC_SVMlinear<- roc(factor(test$TARGET_5Yrs)~p_modelSVMLinear_test[,2], plot = TRUE, print.auc = TRUE)
```
And finally the SVM model with radial fit has AUC=0.718 over the test dataset, which is lower than the SVM model with linear kernel.

```{r ROC for SVMRadial , echo=FALSE, results='asis' , header = FALSE, fig.align='center', fig.cap="ROC for SVM with radial kernel"}
#fig7
ROC_SVMRadial<- roc(factor(test$TARGET_5Yrs)~p_modelSVMRadial_test[,2], plot = TRUE, print.auc = TRUE)
```

Comparing the ROC curves and more specificly the AUC values, it is conculed that SVM fit with linear kernel is the best model among other analyzed fits which has AUC value of 74.2 \%.

\newpage

##Conculsion:

The career longevity of NBA players, with respect to their athletic performance is provided in the analyzed dataset for this project. The outcome is a binary classification based on the years each player stayed in the league >= or < than 5 years. There were 1340 observations and 21 variables. The dataset has been splitted into two "train" and "test" separate dataset. 

The Caret packge in R has been used over the train dataset for training the machine learning algorithms. The boostrap method has been used to control the training with 4 different methods of GLM, glmStepAIC, SVM with linear kernel and SVM with radial Kernel, creating different fits. The accuracy and kappa value are analyzed for all the fitted models. 
The best fits from the different mentioned training methods were then compared with each other, regarding the best prediction over the test dataset. 
Comparisons are made baced on resample() function from caret package, the confusion matrices for both train and test datasets, ROC curves, missclassification error rates and fit accuracy on training data. 
The ROC results show that the best fit among the investigated methods with bootstrap process, is SVM with linear kernel, with AUC value of 74.2\%. Based on the accuracy evaluation made with resample() and as illustrated in Figure 8,SVM with radial kernel with slight improvement w.r.t linear kernel, had higher mean accuracy. The level of mis-classification error is also presented in Table 14, and illustrated in Figure 9.


```{r ETA_ksvm_radial eta, echo=FALSE, warning=FALSE,include=FALSE}
ans <- ksvm(factor(TARGET_5Yrs)~.,data=test, kernel="rbfdot", kpar=list(sigma = 0.06461464), C=0.25)
yH <- predict(ans, newdata=test, type="response")
eta <- mean(factor(test$TARGET_5Yrs)!=yH)
MOE <- 1.96*sqrt(eta*(1-eta)/length(yH))
CI[4,] <- round(100*(eta+c(-1,1)*MOE), 2)
ETA[4] <- eta

```


```{r LINEAR_ksvm, echo=FALSE, warning=FALSE,include=FALSE}
ans <- ksvm(factor(TARGET_5Yrs)~.,data=test, kernel="vanilladot", C=1)
yH <- predict(ans, newdata=test, type="response")
eta <- mean(test$TARGET_5Yrs!=yH)
MOE <- 1.96*sqrt(eta*(1-eta)/length(yH))
CI5 <- round(100*(eta+c(-1,1)*MOE), 2)
CI[3,] <- CI5
ETA[3] <- eta

```


```{r Eta AIC , echo=FALSE, warning=FALSE,include=FALSE}
DMLE2 <- (modelAIC$finalModel)$deviance
DNULL2 <- (modelAIC$finalModel)$null.deviance
n <- nrow(train)
RSq <- (1-exp((DMLE2-DNULL2)/n)) / (1-exp(-DNULL2/n))
px <- predict(modelAIC, newdata=test, type="prob")
yH <- ifelse(px < 0.5, "zero", "one")
y <- factor(test$TARGET_5Yrs)
eta <- mean(y!=yH)
MOE <- 1.96*sqrt(eta*(1-eta)/length(yH))
CI[2,] <- round(100*(eta+c(-1,1)*MOE), 2)
ETA[2] <- eta
```


```{r mis-classification table, echo= FALSE, results='asis' , header = FALSE}
ind <- order(ETA)
#Table 14
names(ETA) <- c("logistic", "AIC", "SVM-linear", "SVM-Radial")
tb <- round(cbind(100*ETA, CI)[ind,],1)
colnames(tb) <- c("error rate %", "lower", "upper")
print(xtable(tb, digits=1))
```





```{r COMPARE_Accuracy, results='asis', echo=FALSE, warning=FALSE, fig.align='center'}

#Figure 8
t <- 1-ETA
ind <- rev(order(t))
imp <- unlist(t)[ind]
var <- names(t)[ind]
tibble(
   var = ordered(var, levels=var), 
   imp = imp,
   moe = 1.96*sqrt(imp*(1-imp)/nrow(train))
 ) %>%
  ggplot(aes(x = var, y = imp)) + 
  geom_bar(stat = "identity", fill="blue") +
  geom_errorbar(aes(ymin=imp-moe, ymax=imp+moe),
                  width=0.5, colour="red", size=2) +
  ggtitle("Accuracy Comparisons with 95% C.I. Error Bars-train set") +
  xlab("ML Algorithm") +
  ylab("Accuracy on Training Data") +
  coord_flip()
```




```{r mis-classification illustration, results='asis', echo=FALSE, warning=FALSE, fig.align='center'}
#Figure 9
ETA2 <- ETA[ind]
var <- names(ETA2)
tibble(
   var = ordered(var, levels=var), #need ordered for Pareto
   imp = c(ETA2)
 ) %>%
  ggplot(aes(x = var, y = imp)) + 
  geom_bar(stat = "identity", fill="blue") +
  ggtitle("Mis-classification Rates") +
  xlab("Algorithm") +
  ylab("Error Rate") +
  coord_flip()
```

























